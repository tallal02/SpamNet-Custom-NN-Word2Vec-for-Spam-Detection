{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Mathematical Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import csv\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Loading and Preprocessing Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *2.1. Load Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    import csv\n",
    "    import sys\n",
    "    # Increase the field size limit\n",
    "    max_int = sys.maxsize\n",
    "    decrement = True\n",
    "    while decrement:\n",
    "        decrement = False\n",
    "        try:\n",
    "            csv.field_size_limit(max_int)\n",
    "        except OverflowError:\n",
    "            max_int = int(max_int / 10)\n",
    "            decrement = True\n",
    "\n",
    "    emails = []\n",
    "    labels = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader, None)\n",
    "        for row in reader:\n",
    "            if len(row) < 2:\n",
    "                continue\n",
    "            email = ','.join(row[:-1])\n",
    "            label = row[-1]\n",
    "            emails.append(email)\n",
    "            labels.append(int(label))\n",
    "    return emails, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *2.2 Preprocess Text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase and remove non-alphabetic characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
    "    words = text.split()\n",
    "    # Remove stopwords\n",
    "    stopwords = set([\n",
    "        'the', 'and', 'is', 'in', 'to', 'of', 'it', 'that', 'this', 'a', 'an',\n",
    "        'for', 'on', 'with', 'as', 'at', 'by', 'from', 'or', 'but', 'if', 'then'\n",
    "    ])\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    # Simple stemming\n",
    "    suffixes = ('ing', 'ed', 's', 'es')\n",
    "    stemmed_words = []\n",
    "    for word in words:\n",
    "        for suffix in suffixes:\n",
    "            if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                word = word[:-len(suffix)]\n",
    "                break\n",
    "        stemmed_words.append(word)\n",
    "    return stemmed_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *2.3 Build Vocabulary*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(tokenized_texts):\n",
    "    vocab = {}\n",
    "    index = 0\n",
    "    for words in tokenized_texts:\n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = index\n",
    "                index += 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *2.4 Balance the Dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(emails, labels):\n",
    "    spam_emails = [(email, label) for email, label in zip(emails, labels) if label == 1]\n",
    "    not_spam_emails = [(email, label) for email, label in zip(emails, labels) if label == 0]\n",
    "    min_count = min(len(spam_emails), len(not_spam_emails))\n",
    "    spam_emails_sampled = spam_emails[:min_count]\n",
    "    not_spam_emails_sampled = not_spam_emails[:min_count]\n",
    "    balanced_data = spam_emails_sampled + not_spam_emails_sampled\n",
    "    random.shuffle(balanced_data)\n",
    "    balanced_emails = [email for email, label in balanced_data]\n",
    "    balanced_labels = [label for email, label in balanced_data]\n",
    "    return balanced_emails, balanced_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *2.5 Main Preprocessing Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(file_path):\n",
    "    emails, labels = load_dataset(file_path)\n",
    "    emails, labels = balance_dataset(emails, labels)\n",
    "    tokenized_texts = [preprocess_text(email) for email in emails]\n",
    "    vocab = build_vocabulary(tokenized_texts)\n",
    "    return tokenized_texts, labels, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Training Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *3.1 Initialize Embeddings*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_embeddings(vocab_size, embedding_dim):\n",
    "    embeddings = {}\n",
    "    for i in range(vocab_size):\n",
    "        embeddings[i] = [random.uniform(-0.5, 0.5) for _ in range(embedding_dim)]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *3.2 Generate Training Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokenized_texts, vocab, window_size):\n",
    "    training_data = []\n",
    "    for words in tokenized_texts:\n",
    "        for idx, word in enumerate(words):\n",
    "            target_word_idx = vocab[word]\n",
    "            context_indices = []\n",
    "            for neighbor in range(max(idx - window_size, 0), min(idx + window_size + 1, len(words))):\n",
    "                if neighbor != idx:\n",
    "                    context_word = words[neighbor]\n",
    "                    context_indices.append(vocab[context_word])\n",
    "            training_data.append((target_word_idx, context_indices))\n",
    "    return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *3.3 Negative Sampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_negative_samples(vocab_size, count):\n",
    "    negative_samples = [random.randint(0, vocab_size - 1) for _ in range(count)]\n",
    "    return negative_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *3.4  Train Word2Vec Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(training_data, vocab_size, embedding_dim, epochs, learning_rate, negative_sample_size):\n",
    "    # Initialize embeddings\n",
    "    W = initialize_embeddings(vocab_size, embedding_dim)\n",
    "    W_prime = initialize_embeddings(vocab_size, embedding_dim)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        random.shuffle(training_data)\n",
    "\n",
    "        for target_word_idx, context_indices in training_data:\n",
    "            # Positive samples\n",
    "            for context_word_idx in context_indices:\n",
    "                label = 1\n",
    "                score = sum(W[target_word_idx][k] * W_prime[context_word_idx][k] for k in range(embedding_dim))\n",
    "                pred = 1 / (1 + math.exp(-score))\n",
    "                pred = max(min(pred, 1 - 1e-7), 1e-7)\n",
    "                loss = -math.log(pred)\n",
    "                total_loss += loss\n",
    "                grad = learning_rate * (label - pred)\n",
    "                for k in range(embedding_dim):\n",
    "                    w_temp = W[target_word_idx][k]\n",
    "                    w_prime_temp = W_prime[context_word_idx][k]\n",
    "                    W[target_word_idx][k] += grad * w_prime_temp\n",
    "                    W_prime[context_word_idx][k] += grad * w_temp\n",
    "\n",
    "            # Negative samples\n",
    "            negative_samples = generate_negative_samples(vocab_size, negative_sample_size)\n",
    "            for negative_word_idx in negative_samples:\n",
    "                label = 0\n",
    "                score = sum(W[target_word_idx][k] * W_prime[negative_word_idx][k] for k in range(embedding_dim))\n",
    "                pred = 1 / (1 + math.exp(-score))\n",
    "                pred = max(min(pred, 1 - 1e-7), 1e-7)\n",
    "                loss = -math.log(1 - pred)\n",
    "                total_loss += loss\n",
    "                grad = learning_rate * (label - pred)\n",
    "                for k in range(embedding_dim):\n",
    "                    w_temp = W[target_word_idx][k]\n",
    "                    w_prime_temp = W_prime[negative_word_idx][k]\n",
    "                    W[target_word_idx][k] += grad * w_prime_temp\n",
    "                    W_prime[negative_word_idx][k] += grad * w_temp\n",
    "\n",
    "        print('Epoch', epoch + 1, 'Loss:', total_loss)\n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Saving Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for idx in embeddings:\n",
    "            vector = embeddings[idx]\n",
    "            vector_str = ' '.join([str(v) for v in vector])\n",
    "            f.write(f'{idx} {vector_str}\\n')\n",
    "\n",
    "def save_vocab(vocab, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for word in vocab:\n",
    "            f.write(f'{word} {vocab[word]}\\n')\n",
    "\n",
    "def save_list(data_list, file_name):\n",
    "    with open(file_name, 'w', encoding='utf-8') as f:\n",
    "        for item in data_list:\n",
    "            if isinstance(item, list):\n",
    "                f.write(' '.join(item) + '\\n')\n",
    "            else:\n",
    "                f.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Running the Word2Vec Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1371294.5840724749\n",
      "Epoch 2 Loss: 1211830.3250119959\n",
      "Epoch 3 Loss: 1088256.054288666\n",
      "Epoch 4 Loss: 984327.2279765035\n",
      "Epoch 5 Loss: 905617.3793770841\n",
      "Epoch 6 Loss: 848571.5099918712\n",
      "Epoch 7 Loss: 811367.4135964148\n",
      "Epoch 8 Loss: 783492.7025554841\n",
      "Epoch 9 Loss: 764442.8687006214\n",
      "Epoch 10 Loss: 748135.8840426878\n",
      "Epoch 11 Loss: 734880.5954946207\n",
      "Epoch 12 Loss: 723378.5425682429\n",
      "Epoch 13 Loss: 714362.0175240496\n",
      "Epoch 14 Loss: 705572.8465825928\n",
      "Epoch 15 Loss: 697286.5573918801\n",
      "Epoch 16 Loss: 690388.2566687807\n",
      "Epoch 17 Loss: 683649.1868091067\n",
      "Epoch 18 Loss: 677116.4908259144\n",
      "Epoch 19 Loss: 672615.7986845918\n",
      "Epoch 20 Loss: 666530.6553002902\n",
      "Epoch 21 Loss: 660850.7973566239\n",
      "Epoch 22 Loss: 655172.8342332322\n",
      "Epoch 23 Loss: 651235.086359787\n",
      "Epoch 24 Loss: 645524.0220935084\n",
      "Epoch 25 Loss: 642038.4870288984\n",
      "Epoch 26 Loss: 637715.6863427298\n",
      "Epoch 27 Loss: 633420.054010779\n",
      "Epoch 28 Loss: 629903.9099397415\n",
      "Epoch 29 Loss: 625406.539674177\n",
      "Epoch 30 Loss: 620970.8727412712\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "embedding_dim = 10\n",
    "window_size = 2\n",
    "epochs = 30\n",
    "learning_rate = 0.005\n",
    "negative_sample_size = 5\n",
    "\n",
    "# Example usage\n",
    "file_path = 'spam_or_not_spam.csv'\n",
    "tokenized_texts, labels, vocab = preprocess_data(file_path)\n",
    "training_data = generate_training_data(tokenized_texts, vocab, window_size)\n",
    "vocab_size = len(vocab)\n",
    "word_embeddings = train_word2vec(training_data, vocab_size, embedding_dim, epochs, learning_rate, negative_sample_size)\n",
    "\n",
    "# Save the data\n",
    "save_embeddings(word_embeddings, 'word_embeddings.txt')\n",
    "save_vocab(vocab, 'vocab.txt')\n",
    "save_list(tokenized_texts, 'tokenized_texts.txt')\n",
    "save_list(labels, 'labels.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
